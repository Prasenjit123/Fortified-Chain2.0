{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r8DMt6jeyZJ"
      },
      "outputs": [],
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1704-9-0-local_9.0.176-1_amd64-deb\n",
        "!ls /var/cuda-repo-9-0-local | grep .pub\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!sudo apt-get install cuda-9.0\n",
        "!pip install thundersvm\n",
        "from thundersvm import SVC\n",
        "!pip install skfeature-chappers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BpPf5-9KfJWa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys \n",
        "import random\n",
        "from sklearn import model_selection, pipeline, preprocessing\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold,StratifiedKFold,GridSearchCV\n",
        "from sklearn.feature_selection import RFE, SelectFromModel, SelectKBest, chi2\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdhgnGW6fNqH"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv('https://raw.githubusercontent.com/Prasenjit123/Fortified-Chain2.0/main/cleveland.csv')\n",
        "dataset_input = dataset.iloc[:, :-1]\n",
        "dataset_label = dataset.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_folds = 10"
      ],
      "metadata": {
        "id": "1VUCU4RPrUj7"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_cross_validation(dataset_input,dataset_label,k):\n",
        "   train_data = []\n",
        "   test_data = []\n",
        "   train_label = []\n",
        "   test_label = []\n",
        "   for i in range(len(dataset_input)):\n",
        "       if i%number_of_folds == k:\n",
        "         test_data.append(dataset_input.iloc[i,:]) \n",
        "         test_label.append(dataset_label.iloc[i]) \n",
        "       else:\n",
        "         train_data.append(dataset_input.iloc[i,:])\n",
        "         train_label.append(dataset_label.iloc[i])\n",
        "   train_data  = np.array(train_data)\n",
        "   train_label = np.array(train_label)\n",
        "   test_data   = np.array(test_data)\n",
        "   test_label  = np.array(test_label)\n",
        "   return (train_data, train_label, test_data, test_label)"
      ],
      "metadata": {
        "id": "e6qCRBrYGz4f"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NB Classifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "avg_acc = 0\n",
        "for k in range(number_of_folds):\n",
        "  train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label, k)\n",
        "  train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "  train_means = np.mean(train_data, axis=0)\n",
        "  train_SDs = np.std(train_data, axis=0)\n",
        "  test_data = (test_data - train_means)/train_SDs\n",
        "\n",
        "  clf_NB = GaussianNB()\n",
        "  clf_NB.fit(train_data_normalized, train_label)\n",
        "  scores = clf_NB.predict(test_data)\n",
        "  avg_acc = avg_acc + round(accuracy_score(test_label, scores)*100,2)   \n",
        "\n",
        "print(f'acc: {round(avg_acc/number_of_folds,2)}')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCou7WNafsLL",
        "outputId": "95d91ef3-44ce-4c6e-b095-1f281a48cdad"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 83.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RF Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "avg_acc = 0\n",
        "for k in range(number_of_folds):\n",
        "  train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label, k)\n",
        "  train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "  train_means = np.mean(train_data, axis=0)\n",
        "  train_SDs = np.std(train_data, axis=0)\n",
        "  test_data = (test_data - train_means)/train_SDs\n",
        "\n",
        "  clf_RF = RF_clf = RandomForestClassifier(n_estimators = 100) \n",
        "  clf_RF.fit(train_data_normalized, train_label)\n",
        "  scores = clf_NB.predict(test_data)\n",
        "  avg_acc = avg_acc + round(accuracy_score(test_label, scores)*100,2)   \n",
        "\n",
        "print(f'acc: {round(avg_acc/number_of_folds,2)}')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elwN_3rJhnSf",
        "outputId": "a24554f2-4676-47c8-94e2-2f05757e5bb0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 85.46\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM Classifier\n",
        "avg_acc = 0\n",
        "for k in range(number_of_folds):\n",
        "  train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label, k)\n",
        "  train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "  train_means = np.mean(train_data, axis=0)\n",
        "  train_SDs = np.std(train_data, axis=0)\n",
        "  test_data = (test_data - train_means)/train_SDs\n",
        "  all_scores = []\n",
        "  for C in [0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    for gamma in [0.0001, 0.001, 0.01, 0.1,1]: \n",
        "      clf_SVM = SVC(kernel='rbf', C=C, gamma=gamma, random_state=0, verbose=True)\n",
        "      clf_SVM.fit(train_data_normalized, train_label)\n",
        "      scores = round((clf_SVM.score(test_data,test_label))*100,2)\n",
        "      all_scores.append(scores)  \n",
        "  avg_acc = avg_acc + np.max(all_scores)    \n",
        "\n",
        "print(f'acc: {round(avg_acc/number_of_folds,2)}')    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lDt0HAyEOs1",
        "outputId": "aa152761-f713-4848-96e7-87d9864fdde0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc: 87.79\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA as feature selector\n",
        "# PCA+SVM Classifier\n",
        "from sklearn.decomposition import PCA\n",
        "avg_accuracies = [] \n",
        "for n in [2,3,4,5,6,7,8,9,10,11,12,13]:\n",
        "  avg_acc = 0\n",
        "  for k in range(number_of_folds):\n",
        "    train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label, k)\n",
        "    train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "    train_means = np.mean(train_data, axis=0)\n",
        "    train_SDs = np.std(train_data, axis=0)\n",
        "    test_data = (test_data - train_means)/train_SDs\n",
        "    \n",
        "    pca = PCA(n_components=n)\n",
        "    train_data_transformed = pca.fit_transform(train_data_normalized)\n",
        "    test_data = pca.fit_transform(test_data)\n",
        "    all_scores = []\n",
        "    for C in [0.01, 0.1, 1, 10, 100, 1000]:\n",
        "      for gamma in [0.0001, 0.001, 0.01, 0.1,1]: \n",
        "        clf_SVM = SVC(kernel='rbf', C=C, gamma=gamma, random_state=0, verbose=True)\n",
        "        clf_SVM.fit(train_data_transformed, train_label)\n",
        "        scores = round((clf_SVM.score(test_data,test_label))*100,2)\n",
        "        all_scores.append(scores)  \n",
        "    #print(np.max(all_scores))\n",
        "    avg_acc = avg_acc + np.max(all_scores)    \n",
        "\n",
        "  print(f'feature = {n}, acc: {avg_acc/number_of_folds}') \n",
        "  avg_accuracies.append(avg_acc/number_of_folds)   \n",
        "\n",
        "print(\"\\n Best Accuracy: \", round(np.max(avg_accuracies),2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIgj8Eb1f428",
        "outputId": "db9f6d56-7193-404b-8689-f93a688a8108"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "feature = 2, acc: 78.818\n",
            "feature = 3, acc: 79.80600000000001\n",
            "feature = 4, acc: 80.42999999999999\n",
            "feature = 5, acc: 80.108\n",
            "feature = 6, acc: 80.10799999999999\n",
            "feature = 7, acc: 80.45199999999998\n",
            "feature = 8, acc: 81.441\n",
            "feature = 9, acc: 80.43\n",
            "feature = 10, acc: 81.41900000000001\n",
            "feature = 11, acc: 80.75200000000001\n",
            "feature = 12, acc: 80.77399999999999\n",
            "feature = 13, acc: 80.763\n",
            "\n",
            " Best Accuracy:  81.44\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJX0NRLXNYG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a76034f-fb06-4320-c2ce-68cd596e9924"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 3 features.\n",
            "feature = 2, acc: 73.828\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n",
            "Fitting estimator with 11 features.\n",
            "Fitting estimator with 10 features.\n",
            "Fitting estimator with 9 features.\n",
            "Fitting estimator with 8 features.\n",
            "Fitting estimator with 7 features.\n",
            "Fitting estimator with 6 features.\n",
            "Fitting estimator with 5 features.\n",
            "Fitting estimator with 4 features.\n",
            "Fitting estimator with 13 features.\n",
            "Fitting estimator with 12 features.\n"
          ]
        }
      ],
      "source": [
        "# Random Forest as feature selector\n",
        "# RF+SVM Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "clf_RF = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=1, criterion = \"entropy\")\n",
        "avg_accuracies = [] \n",
        "for n in [2,3,4,5,6,7,8,9,10,11,12,13]:\n",
        "  rfe = RFE(estimator = clf_RF, n_features_to_select = n, verbose =  3) \n",
        "  avg_acc = 0\n",
        "  for k in range(number_of_folds):\n",
        "    train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label, k)\n",
        "    train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "    train_means = np.mean(train_data, axis=0)\n",
        "    train_SDs = np.std(train_data, axis=0)\n",
        "    test_data = (test_data - train_means)/train_SDs\n",
        "    rfe.fit(train_data_normalized, train_label)\n",
        "    train_data_transformed = rfe.transform(train_data_normalized)\n",
        "    test_data = rfe.transform(test_data)\n",
        "\n",
        "    all_scores = []\n",
        "    for C in [0.01, 0.1, 1, 10, 100, 1000]:\n",
        "      for gamma in [0.0001, 0.001, 0.01, 0.1,1]: \n",
        "        clf_SVM = SVC(kernel='rbf', C=C, gamma=gamma, random_state=0, verbose=True)\n",
        "        clf_SVM.fit(train_data_transformed, train_label)\n",
        "        scores = round((clf_SVM.score(test_data,test_label))*100,2)\n",
        "        all_scores.append(scores)  \n",
        "    #print(np.max(all_scores))\n",
        "    avg_acc = avg_acc + np.max(all_scores)    \n",
        "\n",
        "  print(f'feature = {n}, acc: {avg_acc/number_of_folds}') \n",
        "  avg_accuracies.append(avg_acc/number_of_folds)   \n",
        "\n",
        "print(\"\\n Best Accuracy: \", round(np.max(avg_accuracies),2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnmQ6y8jPYPn"
      },
      "outputs": [],
      "source": [
        "# xgboost as feature selector\n",
        "# xgboost+SVM Classifier\n",
        "from xgboost import XGBClassifier\n",
        "clf_XGB = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1)\n",
        "avg_accuracies = []\n",
        "for n in [2,3,4,5,6,7,8,9,10,11,12,13]:\n",
        "  rfe = RFE(estimator = clf_XGB, n_features_to_select = n, verbose =  3) \n",
        "  avg_acc = 0\n",
        "  for k in range(number_of_folds):\n",
        "     train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label, k)\n",
        "     train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "     train_means = np.mean(train_data, axis=0)\n",
        "     train_SDs = np.std(train_data, axis=0)\n",
        "     test_data = (test_data - train_means)/train_SDs\n",
        "     rfe.fit(train_data_normalized, train_label)\n",
        "     train_data_transformed = rfe.transform(train_data_normalized)\n",
        "     test_data = rfe.transform(test_data)\n",
        "\n",
        "     all_scores = []\n",
        "     for C in [0.01, 0.1, 1, 10, 100, 1000]:\n",
        "       for gamma in [0.0001, 0.001, 0.01, 0.1,1]: \n",
        "         clf_SVM = SVC(kernel='rbf', C=C, gamma=gamma, random_state=0, verbose=True)\n",
        "         clf_SVM.fit(train_data_transformed, train_label)\n",
        "         scores = round((clf_SVM.score(test_data,test_label))*100,2)\n",
        "         all_scores.append(scores)  \n",
        "     #print(np.max(all_scores))\n",
        "     avg_acc = avg_acc + np.max(all_scores)    \n",
        "\n",
        "  print(f'feature = {n}, acc: {avg_acc/number_of_folds}')\n",
        "  avg_accuracies.append(avg_acc/number_of_folds)   \n",
        "\n",
        "print(\"\\n Best Accuracy: \", np.max(avg_accuracies)) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vim2G0USjVTw"
      },
      "outputs": [],
      "source": [
        "# Chi_squre as feature selector\n",
        "# Chi_squre+SVM classifier\n",
        "avg_accuracies = []\n",
        "for n in [2,3,4,5,6,7,8,9,10,11,12,13]:\n",
        "  ch = SelectKBest(score_func=chi2, k=n)\n",
        "  avg_acc = 0\n",
        "  for k in range(number_of_folds):\n",
        "    train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label,k)\n",
        "    train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "    train_data_normalized = MinMaxScaler().fit_transform(train_data_normalized)\n",
        "    train_means = np.mean(train_data, axis=0)\n",
        "    train_SDs = np.std(train_data, axis=0)\n",
        "    test_data = (test_data - train_means)/train_SDs\n",
        "    ch.fit(train_data_normalized, train_label)\n",
        "    train_data_transformed = ch.fit_transform(train_data_normalized, train_label)    \n",
        "     \n",
        "    all_scores = []\n",
        "    for C in [0.01, 0.1, 1, 10, 100, 1000]:\n",
        "      for gamma in [0.0001, 0.001, 0.01, 0.1,1]: \n",
        "        clf_SVM = SVC(kernel='rbf', C=C, gamma=gamma, random_state=0, verbose=True)\n",
        "        clf_SVM.fit(train_data_transformed, train_label)\n",
        "        scores = round((clf_SVM.score(test_data,test_label))*100,2)   \n",
        "        all_scores.append(scores)  \n",
        "    #print(np.max(all_scores))\n",
        "    avg_acc = avg_acc + np.max(all_scores)    \n",
        "\n",
        "  print(f'feature = {n}, acc: {avg_acc/number_of_folds}')  \n",
        "  avg_accuracies.append(avg_acc/number_of_folds)   \n",
        "\n",
        "print(\"\\n Best Accuracy: \", round(np.max(avg_accuracies),2)) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso as feature selector\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "                     ('scaler',StandardScaler()),\n",
        "                     ('model',Lasso())\n",
        "])\n",
        "search = GridSearchCV(pipeline, {'model__alpha':np.arange(0.1,10,0.1)}, cv = 5, scoring=\"neg_mean_squared_error\",verbose=3)\n",
        "\n",
        "avg_acc = 0\n",
        "for k in range(number_of_folds):\n",
        "  train_data, train_label, test_data, test_label = k_fold_cross_validation(dataset_input, dataset_label,k)\n",
        "  train_data_normalized = StandardScaler().fit_transform(train_data)\n",
        "  train_means = np.mean(train_data, axis=0)\n",
        "  train_SDs = np.std(train_data, axis=0)\n",
        "  test_data = (test_data - train_means)/train_SDs\n",
        "  search.fit(train_data_normalized, train_label)\n",
        "  coefficients = search.best_estimator_.named_steps['model'].coef_\n",
        "  importance = np.abs(coefficients)\n",
        "  features = np.arange(13)\n",
        "  selected_features = np.array(features)[importance != 0]\n",
        "  train_data_transformed = train_data_normalized[:,selected_features]\n",
        "  test_data = test_data[:,selected_features]\n",
        "\n",
        "  all_scores = []\n",
        "  for C in [0.01, 0.1, 1, 10, 100, 1000]:\n",
        "    for gamma in [0.0001, 0.001, 0.01, 0.1,1]: \n",
        "      clf_SVM = SVC(kernel='rbf', C=C, gamma=gamma, random_state=0, verbose=True)\n",
        "      clf_SVM.fit(train_data_transformed, train_label)\n",
        "      scores = round((clf_SVM.score(test_data,test_label))*100,2)  \n",
        "      all_scores.append(scores)  \n",
        "  #print(np.max(all_scores))\n",
        "  avg_acc = avg_acc + np.max(all_scores)    \n",
        "\n",
        "print(f'feature = {selected_features}, acc: {round(avg_acc/number_of_folds,2)}')  "
      ],
      "metadata": {
        "id": "BJAvIjfk12gn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RF_SVM_cleveland_k_fold",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}